name: Model deploy
inputs:
- {name: Model, type: Model}
outputs:
- {name: output_model, type: Model}
- {name: Endpoint, type: Artifact}
implementation:
  container:
    image: python:3.8-slim
    command:
    - sh
    - -c
    - |2

      if ! [ -x "$(command -v pip)" ]; then
          python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
      fi

      PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'google-cloud-aiplatform' 'kfp==1.8.9' && "$0" "$@"
    - sh
    - -ec
    - |
      program_path=$(mktemp -d)
      printf "%s" "$0" > "$program_path/ephemeral_component.py"
      python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
    - |2+

      import kfp
      from kfp.v2 import dsl
      from kfp.v2.dsl import *
      from typing import *

      def model_deploy(
          Model: Input[Model],
          output_model: Output[Model],
          Endpoint: Output[Artifact]
      ):
          import pickle
          from google.cloud import aiplatform
          import time


          TIME_STAMP = str(int(time.time()))
          DISPLAY_NAME = f"chicago-prediction-{TIME_STAMP}"
          MODEL_NAME = f"chicago-taxi-model-{TIME_STAMP}"
          ENDPOINT_NAME = f"chicago-taxi-endpoint-{TIME_STAMP}"
          PROJECT = 'niveustraining'
          REGION = 'us-central1'

          aiplatform.init(
              project=PROJECT,
              location=REGION
          )

          model = aiplatform.Model.upload(
              display_name=DISPLAY_NAME,
              artifact_uri=Model.uri.replace('Model', ''),
              serving_container_image_uri="us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-6:latest",
          )



          endpoint = aiplatform.Endpoint.create(
              display_name=ENDPOINT_NAME,
              project=PROJECT,
              location=REGION,
          )

          Endpoint.uri = endpoint.resource_name

          model.deploy(
              endpoint=endpoint,
              deployed_model_display_name=DISPLAY_NAME,
              machine_type="n1-standard-4",
              traffic_split={"0": 100}
          )

          model.wait()

          output_model.uri = model.resource_name

    args:
    - --executor_input
    - {executorInput: null}
    - --function_to_execute
    - model_deploy
